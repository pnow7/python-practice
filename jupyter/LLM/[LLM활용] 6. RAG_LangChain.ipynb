{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "e1dde7ba",
      "metadata": {},
      "source": [
        "# OpenAI API 6 — RAG + LangChain "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cc423f7c",
      "metadata": {},
      "source": [
        "\n",
        "## 학습 목표\n",
        "- RAG(Retrieval-Augmented Generation)의 개념과 구조 이해\n",
        "- OpenAI Embeddings API로 벡터화 및 코사인 유사도 계산 실습\n",
        "- LangChain 설치 및 핵심 컴포넌트(LLM, PromptTemplate, Chain) 이해\n",
        "- FAISS/Chroma 기반 벡터 검색 + RetrievalQA, ConversationalRetrievalChain 구축\n",
        "- PDF 문서 기반 RAG 질의응답 시스템 구현\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2434f1c8",
      "metadata": {},
      "source": [
        "\n",
        "## 1. RAG 개념 및 구조\n",
        "| 항목 | 설명 |\n",
        "|---|---|\n",
        "| 정의 | 외부 지식원(DB/문서/웹)을 검색(Retrieval)해 LLM 생성(Generation)에 반영하는 아키텍처 |\n",
        "| 핵심 구성 | Retriever(검색기) + Generator(생성기) |\n",
        "| 장점 | 최신 정보 반영, 환각 감소, 도메인 특화 정확도 향상 |\n",
        "| 활용 | 사내 위키 Q&A, 매뉴얼/정책 문서 질의응답, 제품 FAQ 챗봇 |\n",
        "\n",
        "기본 흐름:\n",
        "```\n",
        "사용자 질문 → [Retriever] 관련 문서 검색 → [Generator] 문서 기반 답변 생성\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3637e183",
      "metadata": {},
      "source": [
        "\n",
        "## 2. OpenAI Embeddings 실습: 벡터화와 코사인 유사도\n",
        "아래 예제는 텍스트를 임베딩으로 변환하고, 코사인 유사도로 유사도를 계산합니다.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "20af8004",
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "from openai import OpenAI\n",
        "import numpy as np\n",
        "\n",
        "client = OpenAI()\n",
        "\n",
        "texts = [\n",
        "    \"OpenAI는 인공지능 연구소입니다.\",\n",
        "    \"RAG는 검색 기반 생성 모델 구조를 의미합니다.\",\n",
        "    \"벡터 데이터베이스는 문장 임베딩을 저장합니다.\"\n",
        "]\n",
        "\n",
        "# 문장 임베딩 생성\n",
        "embs = []\n",
        "for t in texts:\n",
        "    context_vector = client.embeddings.create(input=t, model=\"text-embedding-3-small\").data[0].embedding\n",
        "    embs.append(context_vector)\n",
        "    print(len(context_vector))\n",
        "    print(context_vector)\n",
        "# embs = [client.embeddings.create(input=t, model=\"text-embedding-3-small\").data[0].embedding for t in texts]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3982315b",
      "metadata": {},
      "outputs": [],
      "source": [
        "def cosine_similarity(a, b):\n",
        "    a = np.array(a); b = np.array(b)\n",
        "    return float(np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b)))\n",
        "\n",
        "# 질의 임베딩\n",
        "query = \"RAG 구조에 대해 설명해줘.\"\n",
        "q_emb = client.embeddings.create(input=query, model=\"text-embedding-3-small\").data[0].embedding\n",
        "\n",
        "# 유사도 계산 및 상위 정렬\n",
        "scores = [cosine_similarity(q_emb, e) for e in embs]\n",
        "for idx, score in sorted(enumerate(scores), key=lambda x: x[1], reverse=True):\n",
        "    print(f\"{score:.3f} : {texts[idx]}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d6c74b45",
      "metadata": {},
      "source": [
        "\n",
        "## 3. LangChain 설치\n",
        "아래 명령으로 필요한 라이브러리를 설치하세요.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "feb405b0",
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# 터미널이나 노트북에서 실행\n",
        "# %pip install langchain openai faiss-cpu chromadb pypdf tiktoken\n",
        "print(\"필요 패키지: langchain, openai, faiss-cpu, chromadb, pypdf, tiktoken\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d1f8db78",
      "metadata": {},
      "source": [
        "\n",
        "## 4. LangChain 기본 구조: LLM + PromptTemplate + Chain\n",
        "LLM, PromptTemplate, Chain을 사용해 간단한 파이프라인을 구성합니다.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bd2553d8",
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.chains import LLMChain\n",
        "\n",
        "llm = ChatOpenAI(model=\"gpt-4o-mini\")\n",
        "\n",
        "template = \"너는 친절한 AI 강사야. 다음 질문에 자세히 답해줘: {question}\"\n",
        "prompt = PromptTemplate(template=template, input_variables=[\"question\"])\n",
        "\n",
        "chain = LLMChain(llm=llm, prompt=prompt)\n",
        "reply = chain.run(\"LangChain의 장점은 무엇인가요?\")\n",
        "print(reply)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "22cd1b99",
      "metadata": {},
      "outputs": [],
      "source": [
        "from IPython.display import Markdown\n",
        "Markdown(reply)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "019c6d3e",
      "metadata": {},
      "source": [
        "\n",
        "## 5. 두 개의 Chain 연결: SimpleSequentialChain\n",
        "한 단계의 결과를 다음 단계 입력으로 연결합니다.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b2748f25",
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "from langchain.chains import SimpleSequentialChain\n",
        "\n",
        "# 1단계: 요약\n",
        "prompt_summary = PromptTemplate(\n",
        "    template=\"다음 텍스트를 한 문장으로 요약해줘:\\n{content}\",\n",
        "    input_variables=[\"content\"]\n",
        ")\n",
        "chain_summary = LLMChain(llm=llm, prompt=prompt_summary) # llm = ChatOpenAI(model=\"gpt-4o-mini\")\n",
        "\n",
        "# 2단계: 영어 번역\n",
        "prompt_translate = PromptTemplate(\n",
        "    template=\"다음 문장을 영어로 번역해줘:\\n{content}\",\n",
        "    input_variables=[\"content\"]\n",
        ")\n",
        "chain_translate = LLMChain(llm=llm, prompt=prompt_translate)\n",
        "\n",
        "# 두 체인을 순차적으로 연결\n",
        "seq_chain = SimpleSequentialChain(chains=[chain_summary, chain_translate])\n",
        "\n",
        "# 예문\n",
        "text = \"\"\"\n",
        "LangChain은 LLM을 활용한 애플리케이션 개발을 위한 강력한 오픈소스 프레임워크입니다.\n",
        "이 라이브러리는 개발자가 프롬프트 관리, 체인 구성, 에이전트 생성 등 복잡한 작업을\n",
        "모듈화된 방식으로 쉽게 처리할 수 있도록 돕습니다. 특히, 외부 데이터 소스를 LLM과\n",
        "연동하는 RAG(검색 증강 생성) 시스템을 구축할 때 그 진가를 발휘하며,\n",
        "이는 AI 챗봇의 답변 정확도를 크게 향상시킬 수 있습니다.\n",
        "\"\"\"\n",
        "\n",
        "# 체인 실행 및 결과 출력\n",
        "print(seq_chain.run(text))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "289087ba",
      "metadata": {},
      "outputs": [],
      "source": [
        "# 요약만 실시\n",
        "chain_summary.run(text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1a05352b",
      "metadata": {},
      "outputs": [],
      "source": [
        "# 번역만 실시\n",
        "chain_translate.run(text)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "109c9d42",
      "metadata": {},
      "source": [
        "\n",
        "## 6. Embedding + FAISS 벡터DB로 유사 검색\n",
        "텍스트를 벡터화하여 FAISS에 저장하고, 질의로 유사한 문서를 검색합니다.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6f58fb29",
      "metadata": {},
      "source": [
        "#### ※ 점수(Score) 해석 방법\n",
        "\n",
        "`similarity_search_with_score`가 반환하는 점수는 일반적인 '유사도'와는 조금 다릅니다. 이 점수는 두 벡터 사이의 **거리(distance)** 를 의미합니다.\n",
        "\n",
        "FAISS의 기본 거리 측정 방식은 **L2 거리(유클리드 거리)** 입니다.\n",
        "\n",
        "  * **점수가 0에 가까울수록 (즉, 작을수록) 더 유사하다**는 의미입니다.\n",
        "  * 반대로 점수가 클수록 두 벡터의 거리가 멀어 유사성이 낮다는 뜻입니다.\n",
        "\n",
        "따라서 위 코드를 실행하면, 검색어와 가장 관련성이 높은 문서가 가장 작은 점수와 함께 출력될 것입니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c224a763",
      "metadata": {},
      "outputs": [],
      "source": [
        "# !pip install langchain_openai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c02aabba",
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain_openai import OpenAIEmbeddings\n",
        "from langchain_community.vectorstores import FAISS\n",
        "\n",
        "# OpenAIEmbeddings 객체 생성\n",
        "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
        "\n",
        "# --- 확장된 문서 리스트 ---\n",
        "docs = [\n",
        "    \"RAG는 검색과 생성을 결합하여 LLM의 답변 정확도를 높이는 모델 구조입니다.\",\n",
        "    \"FAISS는 Meta에서 개발한, 대규모 벡터를 빠르게 검색하기 위한 라이브러리입니다.\",\n",
        "    \"Chroma는 파이썬에서 쉽게 쓸 수 있는 경량 오픈소스 벡터 데이터베이스입니다.\",\n",
        "    \"임베딩은 텍스트나 이미지 같은 데이터를 컴퓨터가 이해할 수 있는 숫자 벡터로 변환하는 과정입니다.\",\n",
        "    \"벡터 저장소는 임베딩된 벡터들을 저장하고, 빠르고 효율적인 유사도 검색을 가능하게 합니다.\",\n",
        "    \"LangChain은 LLM을 활용한 애플리케이션 개발을 돕는 강력한 프레임워크입니다.\",\n",
        "    \"에이전트는 LLM이 스스로 판단하여 외부 도구를 사용하는 등 복잡한 작업을 수행하는 능력을 가집니다.\",\n",
        "    \"파인튜닝은 사전 학습된 거대 언어 모델을 특정 작업이나 도메인에 맞게 추가로 학습시키는 과정입니다.\",\n",
        "    \"시맨틱 검색은 단순한 키워드 일치가 아닌, 문장의 문맥과 의미를 기반으로 검색 결과를 제공합니다.\",\n",
        "    \"허깅페이스는 트랜스포머 기반의 다양한 모델과 데이터셋을 공유하는 거대한 플랫폼입니다.\",\n",
        "    \"Pinecone은 클라우드 기반의 완전 관리형 벡터 데이터베이스 서비스로, 확장성이 뛰어납니다.\",\n",
        "    \"LlamaIndex는 LLM에 외부 데이터를 연결하고 질의하는 것에 특화된 데이터 프레임워크입니다.\"\n",
        "]\n",
        "\n",
        "# Faiss 벡터DB 생성\n",
        "print(\"벡터 데이터베이스를 생성 중입니다...\")\n",
        "db = FAISS.from_texts(docs, embeddings)\n",
        "print(\"생성 완료!\")\n",
        "\n",
        "\n",
        "# --- 다양한 검색어로 테스트 ---\n",
        "queries = [\n",
        "    \"벡터 데이터베이스 종류 알려줘\",\n",
        "    \"LLM한테 외부 지식을 알려주려면 어떻게 해?\",\n",
        "    \"모델을 특정 목적에 맞게 훈련시키는 건 뭐야?\"\n",
        "]\n",
        "\n",
        "for query in queries:\n",
        "    print(\"\\n\" + \"=\"*40)\n",
        "    print(f\"검색어: '{query}'\")\n",
        "    print(\"=\"*40)\n",
        "    \n",
        "    # 유사도 검색 실행 (점수와 함께)\n",
        "    results_with_scores = db.similarity_search_with_score(query, k=3)\n",
        "    \n",
        "    # 결과 출력\n",
        "    for doc, score in results_with_scores:\n",
        "        print(f\"Score: {score:.4f}\") # 소수점 4자리까지 출력\n",
        "        print(f\"Content: {doc.page_content}\")\n",
        "        print(\"-\" * 20)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5e51fa2e",
      "metadata": {},
      "source": [
        "\n",
        "## 7. RetrievalQA로 RAG 파이프라인 구성\n",
        "벡터DB에서 검색한 문맥으로 LLM이 답하도록 구성합니다.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "002db978",
      "metadata": {},
      "source": [
        "* RetrievalQA는 LLM이 엉뚱한 대답(환각)을 하는 대신, 주어진 자료에 근거하여 정확하고 신뢰도 높은 답변을 하도록 유도합니다.\n",
        "\n",
        "### 동작 과정\n",
        "RetrievalQA 체인은 내부적으로 다음과 같은 단계를 거쳐 작동합니다.\n",
        "\n",
        "**1. 질문 입력**: 사용자가 질문을 합니다. (예: \"FAISS의 장점은 무엇인가요?\")\n",
        "\n",
        "**2. 문서 검색 (Retrieve)**:\n",
        "\n",
        "* 질문 문장을 벡터로 변환(임베딩)합니다.\n",
        "\n",
        "* 미리 생성해 둔 벡터 저장소(Vector Store, 예: FAISS)에서 질문 벡터와 가장 유사한(관련성 높은) 문서 조각들을 검색합니다.\n",
        "\n",
        "**3. 프롬프트 구성 (Augment):**\n",
        "\n",
        "* 검색된 문서 조각들과 원래 질문을 조합하여 LLM에게 전달할 새로운 프롬프트(Prompt)를 만듭니다.\n",
        "\n",
        "* 예시:\n",
        "```yaml\n",
        "    [Context]\n",
        "    - FAISS는 Meta에서 개발한... 라이브러리입니다.\n",
        "    - FAISS는 메모리 효율성이 뛰어납니다...\n",
        "\n",
        "    [Question]\n",
        "    FAISS의 장점은 무엇인가요?\n",
        "```\n",
        "**4. 답변 생성 (Generate):**\n",
        "\n",
        "구성된 프롬프트를 LLM(예: GPT-4)에 전달합니다.\n",
        "\n",
        "LLM은 주어진 Context 내용을 바탕으로 질문에 대한 최종 답변을 생성합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4c9f6ff2",
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain.chains import RetrievalQA\n",
        "\n",
        "retriever = db.as_retriever()\n",
        "qa = RetrievalQA.from_chain_type(\n",
        "    llm=llm,\n",
        "    retriever=retriever,\n",
        "    return_source_documents=True\n",
        ")\n",
        "\n",
        "question = \"RAG와 FAISS의 역할을 설명해줘.\"\n",
        "res = qa.invoke(question)\n",
        "res"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "016422a9",
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"답변:\", res[\"result\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c9536940",
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"\\n참고 소스 문서:\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cf6c9046",
      "metadata": {},
      "outputs": [],
      "source": [
        "for d in res[\"source_documents\"]:\n",
        "    print(\"-\", d.page_content)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e298905b",
      "metadata": {},
      "source": [
        "\n",
        "## 8. ConversationalRetrievalChain로 대화형 RAG\n",
        "대화 기록을 메모리로 관리하면서 검색+생성을 수행합니다.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f9ac5f7e",
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "from langchain.chains import ConversationalRetrievalChain\n",
        "from langchain.memory import ConversationBufferMemory\n",
        "\n",
        "memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)\n",
        "conv = ConversationalRetrievalChain.from_llm(\n",
        "    llm=llm,\n",
        "    retriever=db.as_retriever(),\n",
        "    memory=memory\n",
        ")\n",
        "\n",
        "qs = [\"RAG는 무엇인가요?\", \"FAISS는 왜 쓰나요?\", \"둘의 관계를 간단히 요약해줘.\"]\n",
        "for q in qs:\n",
        "    out = conv.invoke({\"question\": q})\n",
        "    print(\"Q:\", q)\n",
        "    print(\"A:\", out[\"answer\"], \"\\n\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ab921349",
      "metadata": {},
      "source": [
        "\n",
        "## 9. PDF 문서 기반 RAG 질의응답\n",
        "PyPDFLoader로 PDF를 로드하고, 벡터DB를 구성해 질의응답을 수행합니다.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1a185255",
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "from langchain.document_loaders import PyPDFLoader\n",
        "\n",
        "# 예시 파일명: example.pdf (같은 폴더에 준비)\n",
        "loader = PyPDFLoader(\"카카오뱅크323410.pdf\")\n",
        "pages = loader.load()\n",
        "\n",
        "texts = [p.page_content for p in pages]\n",
        "pdf_db = FAISS.from_texts(texts, embeddings)\n",
        "pdf_retriever = pdf_db.as_retriever()\n",
        "\n",
        "qa_pdf = RetrievalQA.from_chain_type(llm=llm, retriever=pdf_retriever)\n",
        "print(qa_pdf.invoke(\"이 문서의 핵심 내용을 요약해줘.\")[\"result\"])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c0cc06e7",
      "metadata": {},
      "source": [
        "\n",
        "## 10. 실습 과제\n",
        "1) 자신의 문서를 TXT/PDF로 준비해 벡터DB 구축 후 질의응답을 구성하세요.  \n",
        "2) Retrieval 파라미터(k, score_threshold 등)를 조정해 정확도 변화를 비교하세요.  \n",
        "3) 프롬프트에 인용 표기 형식(예: [출처: 문서제목])을 강제하여 근거를 포함한 답변을 생성해보세요.  \n",
        "\n",
        "## 참고 자료\n",
        "- OpenAI Embeddings API: https://platform.openai.com/docs/api-reference/embeddings\n",
        "- LangChain 공식 문서: https://python.langchain.com/docs/\n",
        "- FAISS: https://faiss.ai/\n",
        "- LangChain PDF QA 튜토리얼: https://python.langchain.com/docs/tutorials/pdf_qa\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "py310_llm",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
